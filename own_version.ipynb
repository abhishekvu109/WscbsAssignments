{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in ./venv/lib/python3.10/site-packages (0.17.1)\r\n",
      "Requirement already satisfied: nltk>=3.1 in ./venv/lib/python3.10/site-packages (from textblob) (3.7)\r\n",
      "Requirement already satisfied: click in ./venv/lib/python3.10/site-packages (from nltk>=3.1->textblob) (8.1.3)\r\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.10/site-packages (from nltk>=3.1->textblob) (1.1.0)\r\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.10/site-packages (from nltk>=3.1->textblob) (4.64.0)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./venv/lib64/python3.10/site-packages (from nltk>=3.1->textblob) (2022.4.24)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.1 is available.\r\n",
      "You should consider upgrading via the '/home/am-i-helpful/PycharmProjects/Sentiment Analysis - COVID-19 Vaccine Tweets/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: wordcloud in ./venv/lib64/python3.10/site-packages (1.8.1)\r\n",
      "Requirement already satisfied: pillow in ./venv/lib64/python3.10/site-packages (from wordcloud) (9.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.6.1 in ./venv/lib64/python3.10/site-packages (from wordcloud) (1.22.3)\r\n",
      "Requirement already satisfied: matplotlib in ./venv/lib64/python3.10/site-packages (from wordcloud) (3.5.2)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./venv/lib64/python3.10/site-packages (from matplotlib->wordcloud) (1.4.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./venv/lib/python3.10/site-packages (from matplotlib->wordcloud) (3.0.9)\r\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.10/site-packages (from matplotlib->wordcloud) (0.11.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from matplotlib->wordcloud) (21.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.10/site-packages (from matplotlib->wordcloud) (2.8.2)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.10/site-packages (from matplotlib->wordcloud) (4.33.3)\r\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.1 is available.\r\n",
      "You should consider upgrading via the '/home/am-i-helpful/PycharmProjects/Sentiment Analysis - COVID-19 Vaccine Tweets/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "[nltk_data] Downloading package brown to /home/am-i-\r\n",
      "[nltk_data]     helpful/nltk_data...\r\n",
      "[nltk_data]   Package brown is already up-to-date!\r\n",
      "[nltk_data] Downloading package punkt to /home/am-i-\r\n",
      "[nltk_data]     helpful/nltk_data...\r\n",
      "[nltk_data]   Package punkt is already up-to-date!\r\n",
      "[nltk_data] Downloading package wordnet to /home/am-i-\r\n",
      "[nltk_data]     helpful/nltk_data...\r\n",
      "[nltk_data]   Package wordnet is already up-to-date!\r\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\r\n",
      "[nltk_data]     /home/am-i-helpful/nltk_data...\r\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\r\n",
      "[nltk_data]       date!\r\n",
      "[nltk_data] Downloading package conll2000 to /home/am-i-\r\n",
      "[nltk_data]     helpful/nltk_data...\r\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\r\n",
      "[nltk_data] Downloading package movie_reviews to /home/am-i-\r\n",
      "[nltk_data]     helpful/nltk_data...\r\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\r\n",
      "Finished.\r\n",
      "Requirement already satisfied: plotly in ./venv/lib/python3.10/site-packages (5.8.0)\r\n",
      "Requirement already satisfied: tenacity>=6.2.0 in ./venv/lib/python3.10/site-packages (from plotly) (8.0.1)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.1 is available.\r\n",
      "You should consider upgrading via the '/home/am-i-helpful/PycharmProjects/Sentiment Analysis - COVID-19 Vaccine Tweets/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: wordninja in ./venv/lib/python3.10/site-packages (2.0.0)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.1 is available.\r\n",
      "You should consider upgrading via the '/home/am-i-helpful/PycharmProjects/Sentiment Analysis - COVID-19 Vaccine Tweets/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: pyspellchecker in ./venv/lib/python3.10/site-packages (0.6.3)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.1 is available.\r\n",
      "You should consider upgrading via the '/home/am-i-helpful/PycharmProjects/Sentiment Analysis - COVID-19 Vaccine Tweets/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: pandas in ./venv/lib64/python3.10/site-packages (1.4.2)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./venv/lib64/python3.10/site-packages (from pandas) (1.22.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./venv/lib/python3.10/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.10/site-packages (from pandas) (2022.1)\r\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.1 is available.\r\n",
      "You should consider upgrading via the '/home/am-i-helpful/PycharmProjects/Sentiment Analysis - COVID-19 Vaccine Tweets/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: seaborn in ./venv/lib/python3.10/site-packages (0.11.2)\r\n",
      "Requirement already satisfied: numpy>=1.15 in ./venv/lib64/python3.10/site-packages (from seaborn) (1.22.3)\r\n",
      "Requirement already satisfied: matplotlib>=2.2 in ./venv/lib64/python3.10/site-packages (from seaborn) (3.5.2)\r\n",
      "Requirement already satisfied: scipy>=1.0 in ./venv/lib64/python3.10/site-packages (from seaborn) (1.8.0)\r\n",
      "Requirement already satisfied: pandas>=0.23 in ./venv/lib64/python3.10/site-packages (from seaborn) (1.4.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from matplotlib>=2.2->seaborn) (21.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.10/site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./venv/lib/python3.10/site-packages (from matplotlib>=2.2->seaborn) (3.0.9)\r\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.10/site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./venv/lib64/python3.10/site-packages (from matplotlib>=2.2->seaborn) (1.4.2)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./venv/lib64/python3.10/site-packages (from matplotlib>=2.2->seaborn) (9.1.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.10/site-packages (from matplotlib>=2.2->seaborn) (4.33.3)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.10/site-packages (from pandas>=0.23->seaborn) (2022.1)\r\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.1 is available.\r\n",
      "You should consider upgrading via the '/home/am-i-helpful/PycharmProjects/Sentiment Analysis - COVID-19 Vaccine Tweets/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: tensorflow in ./venv/lib64/python3.10/site-packages (2.8.0)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./venv/lib64/python3.10/site-packages (from tensorflow) (0.25.0)\r\n",
      "Requirement already satisfied: absl-py>=0.4.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (1.0.0)\r\n",
      "Requirement already satisfied: numpy>=1.20 in ./venv/lib64/python3.10/site-packages (from tensorflow) (1.22.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./venv/lib/python3.10/site-packages (from tensorflow) (4.2.0)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in ./venv/lib64/python3.10/site-packages (from tensorflow) (3.6.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (1.6.3)\r\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in ./venv/lib/python3.10/site-packages (from tensorflow) (2.8.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (1.1.0)\r\n",
      "Requirement already satisfied: flatbuffers>=1.12 in ./venv/lib/python3.10/site-packages (from tensorflow) (2.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./venv/lib/python3.10/site-packages (from tensorflow) (3.3.0)\r\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in ./venv/lib/python3.10/site-packages (from tensorflow) (2.8.0.dev2021122109)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./venv/lib64/python3.10/site-packages (from tensorflow) (1.14.1)\r\n",
      "Requirement already satisfied: six>=1.12.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (1.16.0)\r\n",
      "Requirement already satisfied: protobuf>=3.9.2 in ./venv/lib64/python3.10/site-packages (from tensorflow) (3.20.1)\r\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.10/site-packages (from tensorflow) (60.2.0)\r\n",
      "Requirement already satisfied: libclang>=9.0.1 in ./venv/lib/python3.10/site-packages (from tensorflow) (14.0.1)\r\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in ./venv/lib/python3.10/site-packages (from tensorflow) (1.1.2)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./venv/lib64/python3.10/site-packages (from tensorflow) (1.46.1)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./venv/lib/python3.10/site-packages (from tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in ./venv/lib/python3.10/site-packages (from tensorflow) (2.8.0)\r\n",
      "Requirement already satisfied: gast>=0.2.1 in ./venv/lib/python3.10/site-packages (from tensorflow) (0.5.3)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./venv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./venv/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./venv/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.7)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./venv/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venv/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.27.1)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./venv/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in ./venv/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.1.2)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./venv/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.6.6)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./venv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./venv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (5.0.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./venv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./venv/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.9)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.12)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.3)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./venv/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./venv/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.1 is available.\r\n",
      "You should consider upgrading via the '/home/am-i-helpful/PycharmProjects/Sentiment Analysis - COVID-19 Vaccine Tweets/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: sklearn in ./venv/lib/python3.10/site-packages (0.0)\r\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib64/python3.10/site-packages (from sklearn) (1.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./venv/lib64/python3.10/site-packages (from scikit-learn->sklearn) (1.22.3)\r\n",
      "Requirement already satisfied: joblib>=1.0.0 in ./venv/lib/python3.10/site-packages (from scikit-learn->sklearn) (1.1.0)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in ./venv/lib64/python3.10/site-packages (from scikit-learn->sklearn) (1.8.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv/lib/python3.10/site-packages (from scikit-learn->sklearn) (3.1.0)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.1 is available.\r\n",
      "You should consider upgrading via the '/home/am-i-helpful/PycharmProjects/Sentiment Analysis - COVID-19 Vaccine Tweets/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "!pip install wordcloud\n",
    "!python -m textblob.download_corpora\n",
    "!pip install plotly\n",
    "!pip install wordninja\n",
    "!pip install pyspellchecker\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install tensorflow\n",
    "!pip install sklearn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-15 23:12:05.866690: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-15 23:12:05.866751: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from nltk.stem import SnowballStemmer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "SUPPPORTED_LANG_STEMMER = {\n",
    "    'SPA': SnowballStemmer('spanish'),\n",
    "    'ENG': SnowballStemmer('english'),\n",
    "    'PRT': SnowballStemmer('portuguese'),\n",
    "}\n",
    "\n",
    "\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "\n",
    "    return input_txt\n",
    "\n",
    "def rm_pun_num_esp_cha(pandas_input):\n",
    "    return pandas_input.str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "def rm_esp_cha(pandas_input):\n",
    "    return pandas_input.str.replace(\"[^a-zA-Z0-9áéíóúÁÉÍÓÚâêîôÂÊÎÔãõÃÕñçÇ: ]\", \" \")\n",
    "\n",
    "def rm_length_word(input_data, word_length=3):\n",
    "    return input_data.apply(lambda x: ' '.join([w for w in x.split() if len(w) > word_length]))\n",
    "\n",
    "def tokenize(input_data):\n",
    "    return input_data.apply(lambda x: x.split())\n",
    "\n",
    "def _check_lang(lang):\n",
    "    if lang in SUPPPORTED_LANG_STEMMER:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def stemmer(input_data, language='ENG'):\n",
    "    if  _check_lang(language):\n",
    "        stemmer = SUPPPORTED_LANG_STEMMER[language]\n",
    "        return input_data.apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "    else:\n",
    "        raise \"Language {} not supported for stemming\".format(language)\n",
    "\n",
    "def join_tokenize(input_data, join_char=' '):\n",
    "    return input_data.apply(lambda x: join_char.join(x))\n",
    "\n",
    "def hashtag_extract(input_data, flatten=True):\n",
    "    hashtags = []\n",
    "    for i in input_data:\n",
    "        ht = re.findall(r\"#(\\w+)\", i)\n",
    "        if flatten:\n",
    "            hashtags.append(ht)\n",
    "        else:\n",
    "            hashtags.append([ht])\n",
    "\n",
    "    return sum(hashtags, [])\n",
    "\n",
    "def hashtag_rm(input_data):\n",
    "    return input_data.replace('#', '')\n",
    "# source: https://scorrea92.medium.com/nlp-twitter-sentiment-analysis-with-tensorflow-15e1b2594cfa\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4939/2486901549.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_data = train.append(test, ignore_index=True, sort=True)\n",
      "/tmp/ipykernel_4939/2333447509.py:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  return pandas_input.str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "data": {
      "text/plain": "   id  label                                              tweet  \\\n0   1    0.0   @user when a father is dysfunctional and is s...   \n1   2    0.0  @user @user thanks for #lyft credit i can't us...   \n2   3    0.0                                bihday your majesty   \n3   4    0.0  #model   i love u take with u all the time in ...   \n4   5    0.0             factsguide: society now    #motivation   \n\n                                          tidy_tweet  \\\n0  when father dysfunct selfish drag kid into dys...   \n1     thank  credit caus they offer wheelchair van     \n2                                bihday your majesti   \n3                                love take with time   \n4                                 factsguid societi    \n\n                       hashtag  Name Length  \n0                        [run]           52  \n1  [lyft, disapoint, getthank]           46  \n2                           []           19  \n3                      [model]           20  \n4                      [motiv]           18  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>label</th>\n      <th>tweet</th>\n      <th>tidy_tweet</th>\n      <th>hashtag</th>\n      <th>Name Length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.0</td>\n      <td>@user when a father is dysfunctional and is s...</td>\n      <td>when father dysfunct selfish drag kid into dys...</td>\n      <td>[run]</td>\n      <td>52</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.0</td>\n      <td>@user @user thanks for #lyft credit i can't us...</td>\n      <td>thank  credit caus they offer wheelchair van</td>\n      <td>[lyft, disapoint, getthank]</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0.0</td>\n      <td>bihday your majesty</td>\n      <td>bihday your majesti</td>\n      <td>[]</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0.0</td>\n      <td>#model   i love u take with u all the time in ...</td>\n      <td>love take with time</td>\n      <td>[model]</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0.0</td>\n      <td>factsguide: society now    #motivation</td>\n      <td>factsguid societi</td>\n      <td>[motiv]</td>\n      <td>18</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = './train_E6oV3lV.csv'\n",
    "test_path = './test_tweets_anuFYb8.csv'\n",
    "train  = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "\n",
    "all_data = train.append(test, ignore_index=True, sort=True)\n",
    "all_data['tidy_tweet'] = np.vectorize(remove_pattern)(all_data['tweet'], \"@[\\w]*\")\n",
    "all_data['tidy_tweet'] = rm_pun_num_esp_cha(all_data['tidy_tweet'])\n",
    "all_data['tidy_tweet'] = rm_length_word(all_data['tidy_tweet'])\n",
    "tokenized_tweet = tokenize(all_data['tidy_tweet'])\n",
    "\n",
    "tokenized_tweet = stemmer(tokenized_tweet)\n",
    "all_data['tidy_tweet'] = join_tokenize(tokenized_tweet)\n",
    "all_data['hashtag'] = hashtag_extract(all_data['tidy_tweet'], flatten=False)\n",
    "all_data['tidy_tweet'] = np.vectorize(remove_pattern)(all_data['tidy_tweet'], \"#[\\w]*\")\n",
    "\n",
    "all_data[\"Name Length\"] = all_data['tidy_tweet'].str.len()\n",
    "all_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfVectorizer\n",
    ")\n",
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "# Path to train and test files\n",
    "train_path = 'train_E6oV3lV.csv'\n",
    "test_path = 'test_tweets_anuFYb8.csv'\n",
    "\n",
    "train  = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "\n",
    "# Processed data\n",
    "df = pd.read_csv('./pandas_data_frame.csv', index_col=0)\n",
    "all_data = df.where((pd.notnull(df)), '')\n",
    "\n",
    "# bag-of-words feature matrix\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "bow = bow_vectorizer.fit_transform(all_data['tidy_tweet'])\n",
    "\n",
    "# TF-IDF feature matrix\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(all_data['tidy_tweet'])\n",
    "\n",
    "# splitting data into training and validation set\n",
    "train_bow = bow[:31962,:]\n",
    "test_bow = bow[31962:,:]\n",
    "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train['label'], random_state=42, test_size=0.3)\n",
    "\n",
    "train_tfidf = tfidf[:31962,:]\n",
    "test_tfidf = tfidf[31962:,:]\n",
    "\n",
    "xtrain_tfidf = train_tfidf[ytrain.index]\n",
    "xvalid_tfidf = train_tfidf[yvalid.index]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4939/2748932146.py:12: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  prediction_int = prediction_int.astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with BOW f1: 0.40409683426443205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4939/2748932146.py:20: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  prediction_int = prediction_int.astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with TF-IDF f1: 0.396039603960396\n",
      "Decision Tree Classifier with TF-IDF f1: 0.4157706093189964\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import tree\n",
    "\n",
    "# Logistic Regression with BOW\n",
    "lreg = LogisticRegression()\n",
    "lreg.fit(xtrain_bow, ytrain) # training the model\n",
    "\n",
    "prediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set\n",
    "prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "print(\"Logistic Regression with BOW f1: {}\".format(f1_score(yvalid, prediction_int)))\n",
    "\n",
    "# Logistic Regression with TF-IDF\n",
    "lreg.fit(xtrain_tfidf, ytrain)\n",
    "\n",
    "prediction = lreg.predict_proba(xvalid_tfidf)\n",
    "prediction_int = prediction[:,1] >= 0.3\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "print(\"Logistic Regression with TF-IDF f1: {}\".format(f1_score(yvalid, prediction_int)))\n",
    "\n",
    "\"\"\"Decision Tree Classifier with TF-ID\"\"\"\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(xtrain_tfidf, ytrain)\n",
    "y_pred = clf.predict(xvalid_tfidf)\n",
    "print(\"Decision Tree Classifier with TF-IDF f1: {}\".format(f1_score(yvalid, y_pred)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25569, 120) (25569,)\n",
      "(6393, 120) (6393,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "df = pd.read_csv('./pandas_data_frame.csv', index_col=0)\n",
    "all_data = df.where((pd.notnull(df)), '')\n",
    "all_data['hashtag'] = all_data['hashtag'].apply(literal_eval)\n",
    "\n",
    "full_text = all_data['tidy_tweet'][(all_data['label']=='1.0') | (all_data['label']=='0.0')]\n",
    "y = all_data['label'][(all_data['label']=='1.0') | (all_data['label']=='0.0')]\n",
    "tk = Tokenizer(lower=True, filters='')\n",
    "tk.fit_on_texts(full_text)\n",
    "\n",
    "max_len = 120 # Calculate as max in dataset see 1.data_process.ipynb\n",
    "train_tokenized = tk.texts_to_sequences(full_text)\n",
    "X = pad_sequences(train_tokenized, maxlen=max_len)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, y, random_state=1992, test_size=0.2)\n",
    "\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_val.shape,y_val.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-15 23:13:16.912942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-15 23:13:16.913856: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-15 23:13:16.914124: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-05-15 23:13:16.914376: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-05-15 23:13:16.914601: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-05-15 23:13:16.914818: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-05-15 23:13:16.915042: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-05-15 23:13:16.915252: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-05-15 23:13:16.915482: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-05-15 23:13:16.915511: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-05-15 23:13:16.936483: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 120, 150)          2994000   \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 120, 150)         0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 200)               280800    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 201       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,275,001\n",
      "Trainable params: 3,275,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 150\n",
    "lstm_out = 200\n",
    "max_features = X.max() + 1\n",
    "learnRate = 0.001\n",
    "warmup_epoch = 20\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(max_features, embed_dim, input_length = X.shape[1]))\n",
    "model.add(tf.keras.layers.SpatialDropout1D(0.3))\n",
    "model.add(tf.keras.layers.LSTM(lstm_out, dropout=0.5, recurrent_dropout=0.5))\n",
    "model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss = 'binary_crosentropy',\n",
    "              optimizer='adam',\n",
    "              metrics = ['accuracy', 'f1'])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 120) for input KerasTensor(type_spec=TensorSpec(shape=(None, 120), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None,).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/am-i-helpful/PycharmProjects/Sentiment Analysis - COVID-19 Vaccine Tweets/venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1801, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/am-i-helpful/PycharmProjects/Sentiment Analysis - COVID-19 Vaccine Tweets/venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1790, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/am-i-helpful/PycharmProjects/Sentiment Analysis - COVID-19 Vaccine Tweets/venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1783, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/am-i-helpful/PycharmProjects/Sentiment Analysis - COVID-19 Vaccine Tweets/venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1751, in predict_step\n        return self(x, training=False)\n    File \"/home/am-i-helpful/PycharmProjects/Sentiment Analysis - COVID-19 Vaccine Tweets/venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/am-i-helpful/PycharmProjects/Sentiment Analysis - COVID-19 Vaccine Tweets/venv/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 214, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"sequential\" (type Sequential).\n    \n    Input 0 of layer \"spatial_dropout1d\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 150)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None,), dtype=string)\n      • training=False\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m sample_text \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThe movie was cool.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m predictions \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43msample_text\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(predictions[\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[0;32m~/PycharmProjects/Sentiment Analysis - COVID-19 Vaccine Tweets/venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/PycharmProjects/Sentiment Analysis - COVID-19 Vaccine Tweets/venv/lib64/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1147\u001B[0m, in \u001B[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1145\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint:disable=broad-except\u001B[39;00m\n\u001B[1;32m   1146\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mag_error_metadata\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1147\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mag_error_metadata\u001B[38;5;241m.\u001B[39mto_exception(e)\n\u001B[1;32m   1148\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1149\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mValueError\u001B[0m: in user code:\n\n    File \"/home/am-i-helpful/PycharmProjects/Sentiment Analysis - COVID-19 Vaccine Tweets/venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1801, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/am-i-helpful/PycharmProjects/Sentiment Analysis - COVID-19 Vaccine Tweets/venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1790, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/am-i-helpful/PycharmProjects/Sentiment Analysis - COVID-19 Vaccine Tweets/venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1783, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/am-i-helpful/PycharmProjects/Sentiment Analysis - COVID-19 Vaccine Tweets/venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1751, in predict_step\n        return self(x, training=False)\n    File \"/home/am-i-helpful/PycharmProjects/Sentiment Analysis - COVID-19 Vaccine Tweets/venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/am-i-helpful/PycharmProjects/Sentiment Analysis - COVID-19 Vaccine Tweets/venv/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 214, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"sequential\" (type Sequential).\n    \n    Input 0 of layer \"spatial_dropout1d\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 150)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None,), dtype=string)\n      • training=False\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "sample_text = ('The movie was cool.')\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"vaccination_all_tweets.csv\", quotechar='\"', delimiter=',').dropna()\n",
    "# dropna() for removing the missing values\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "print(df.shape)\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data pre-processing\n",
    "df.drop(columns=['id'], inplace=True)\n",
    "df = df.drop_duplicates('text')\n",
    "print(df.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use regular expressions to strip each tweet of mentions, hashtags, retweet information, and links\n",
    "def clean_tweet_text(text):\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = re.sub(r'RT[\\s]+', '', text)\n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text)\n",
    "    text = text.lower()\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# the following line makes use of an apply function-- it will call clean_tweet_text on every element in the 'text' column\n",
    "df['text'].transform(clean_tweet_text)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis\n",
    "df. describe()\n",
    "# Descriptive statistics include those that summarize the central tendency, dispersion and shape of a dataset’s distribution, excluding NaN values."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# which device are people tweeting about the vaccine from?\n",
    "df['source'].value_counts().head(n=5).plot.bar()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['user_verified'].value_counts().head(n=10).plot.bar()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[df['user_verified'] == True].head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# what are the top 10 most retweeted tweets\n",
    "pd.set_option('display.max_colwidth', 400)\n",
    "df.sort_values(by='retweets', ascending=False)[['text', 'date', 'user_name', 'user_location', 'hashtags', 'favorites', 'retweets']].head(n=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.sort_values(by=['date', 'favorites'], ascending=[True, False])[['text', 'date', 'user_name', 'user_location', 'hashtags', 'favorites', 'retweets']].head(n=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "testimonial = TextBlob(\"So excited to get my vaccine!\")\n",
    "print(testimonial.sentiment)\n",
    "testimonial = TextBlob(\"Is the vaccine painful?\")\n",
    "print(testimonial.sentiment)\n",
    "testimonial = TextBlob(\"The Pfizer vaccine is now FDA approved\")\n",
    "print(testimonial.sentiment)\n",
    "\n",
    "# testing my own random words for fun\n",
    "testimonial = TextBlob(\"What the fuck!\")\n",
    "print(testimonial.sentiment)\n",
    "testimonial = TextBlob(\"Oh my goodness!\")\n",
    "print(testimonial.sentiment)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# let's apply the TextBlob API onto our tweet data to perform sentiment analysis!\n",
    "df['polarity'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "df['subjectivity'] = df['text'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "df['polarity'].hist()\n",
    "plt.xlabel('Polarity Score', fontsize=18)\n",
    "plt.ylabel('Frequency', fontsize=18)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "fig.savefig(\"./polarity_hist.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "df['subjectivity'].hist()\n",
    "plt.xlabel('Subjectivity Score', fontsize=18)\n",
    "plt.ylabel('Frequency', fontsize=18)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "fig.savefig(\"./subjectivity_hist.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# inspect the most negatively charged tweets\n",
    "df.sort_values(by='polarity', ascending=True)[['text', 'polarity', 'subjectivity']].reset_index(drop=True).head(n=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# inspect the most positively charged tweets\n",
    "df.sort_values(by='polarity', ascending=False)[['text', 'polarity', 'subjectivity']].reset_index(drop=True).head(n=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# inspect the most subjective tweets (NOTE: subjectivity scale ranges from 0 to 1)\n",
    "df.sort_values(by='subjectivity', ascending=True)[['text', 'polarity', 'subjectivity']].reset_index(drop=True).head(n=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# inspect the most objective tweets\n",
    "df.sort_values(by='subjectivity', ascending=False)[['text', 'polarity', 'subjectivity']].reset_index(drop=True).head(n=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# let's inspect how many tweets there were with respect to time\n",
    "timeline = df.groupby(['date']).count().reset_index()\n",
    "timeline['count'] = timeline['text']\n",
    "timeline = timeline[['date', 'count']]\n",
    "fig = px.bar(timeline, x='date', y='count', labels={'date': 'Date', 'count': 'Tweet Count'})\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Time-Series Sentiment Analysis\n",
    "# polarity values ranging from -1 to 1 are really useful for sentiment analysis\n",
    "# but let's convert our data to 3 classes (negative, neutral, and positive) so that we can visualize it\n",
    "criteria = [df['polarity'].between(-1, -0.01), df['polarity'].between(-0.01, 0.01), df['polarity'].between(0.01, 1)]\n",
    "values = ['negative', 'neutral', 'positive']\n",
    "df['sentiment'] = np.select(criteria, values, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot sentiment counts\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "df['sentiment'].value_counts().sort_index().plot.bar()\n",
    "plt.xlabel('Sentiment Label', fontsize=18)\n",
    "plt.ylabel('Tweet Count', fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"./sentiment_value_counts\", bbox_inches='tight');"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "timeline = df.groupby(['date']).agg(np.nanmean).reset_index()\n",
    "timeline['count'] = df.groupby(['date']).count().reset_index()['retweets']\n",
    "timeline = timeline[['date', 'count', 'polarity', 'retweets', 'favorites', 'subjectivity']]\n",
    "timeline[\"polarity\"] = timeline[\"polarity\"].astype(float)\n",
    "timeline[\"subjectivity\"] = timeline[\"subjectivity\"].astype(float)\n",
    "timeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot tweets over time, color-coded by average polarity score\n",
    "fig = px.bar(timeline, x='date', y='count', color='polarity')\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Plot tweets over time, color-coded by average subjectivity score\n",
    "fig = px.bar(timeline, x='date', y='count', color='subjectivity')\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_vax = ['covaxin', 'sinopharm', 'sinovac', 'moderna', 'pfizer', 'biontech', 'oxford', 'astrazeneca', 'sputnik']\n",
    "\n",
    "# Function to filter the data to a single vaccine and plot the timeline\n",
    "# Note: a lot of the tweets seem to contain hashtags for multiple vaccines even though they are specifically referring to one vaccine-- not very helpful!\n",
    "def filter_by_vaccy(df, vax):\n",
    "    df_filt = pd.DataFrame()\n",
    "    for v in vax:\n",
    "        df_filt = df_filt.append(df[df['text'].str.lower().str.contains(v)])\n",
    "    other_vax = list(set(all_vax)-set(vax))\n",
    "    for o in other_vax:\n",
    "        df_filt = df_filt[~df_filt['text'].str.lower().str.contains(o)]\n",
    "#     df_filt = df_filt.drop_duplicates()\n",
    "    timeline = df_filt.groupby(['date']).agg(np.nanmean).reset_index()\n",
    "    timeline['count'] = df_filt.groupby(['date']).count().reset_index()['retweets']\n",
    "    timeline = timeline[['date', 'count', 'polarity', 'retweets', 'favorites', 'subjectivity']]\n",
    "    timeline[\"polarity\"] = timeline[\"polarity\"].astype(float)\n",
    "    timeline[\"subjectivity\"] = timeline[\"subjectivity\"].astype(float)\n",
    "    return df_filt, timeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pfizy_df, pfizy_timeline = filter_by_vaccy(df, ['pfizer', 'biontech'])\n",
    "print(pfizy_df.shape)\n",
    "fig = px.bar(pfizy_timeline, x='date', y='count', color='polarity')\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "moderna_df, moderna_timeline = filter_by_vaccy(df, ['moderna'])\n",
    "print(moderna_df.shape)\n",
    "fig = px.bar(moderna_timeline, x='date', y='count', color='polarity')\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Word-clouds section\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import wordninja\n",
    "from spellchecker import SpellChecker\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import math\n",
    "import random\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add(\"amp\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Simple word-cloud\n",
    "tweet_df = pfizy_df\n",
    "words = ' '.join([word for word in tweet_df['text']])\n",
    "word_cloud = WordCloud(width=1000, height=500, random_state=20, max_font_size=120).generate(words)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis('off')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Advanced word-cloud (positive, negative and neutral separation)\n",
    "def flatten_list(l):\n",
    "    return [x for y in l for x in y]\n",
    "\n",
    "def is_acceptable(word: str):\n",
    "    return word not in stop_words and len(word) > 2\n",
    "\n",
    "# Color coding our wordclouds\n",
    "def red_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\n",
    "    return f\"hsl(0, 100%, {random.randint(25, 75)}%)\"\n",
    "\n",
    "def green_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\n",
    "    return f\"hsl({random.randint(90, 150)}, 100%, 30%)\"\n",
    "\n",
    "def yellow_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\n",
    "    return f\"hsl(42, 100%, {random.randint(25, 50)}%)\"\n",
    "\n",
    "# Reusable function to generate word clouds\n",
    "def generate_word_clouds(neg_doc, neu_doc, pos_doc):\n",
    "    # Display the generated image:\n",
    "    fig, axes = plt.subplots(1,3, figsize=(20,10))\n",
    "\n",
    "    wordcloud_neg = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(\" \".join(neg_doc))\n",
    "    axes[0].imshow(wordcloud_neg.recolor(color_func=red_color_func, random_state=3), interpolation='bilinear')\n",
    "    axes[0].set_title(\"Negative Words\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    wordcloud_neu = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(\" \".join(neu_doc))\n",
    "    axes[1].imshow(wordcloud_neu.recolor(color_func=yellow_color_func, random_state=3), interpolation='bilinear')\n",
    "    axes[1].set_title(\"Neutral Words\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    wordcloud_pos = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(\" \".join(pos_doc))\n",
    "    axes[2].imshow(wordcloud_pos.recolor(color_func=green_color_func, random_state=3), interpolation='bilinear')\n",
    "    axes[2].set_title(\"Positive Words\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "#     plt.show();\n",
    "    return fig\n",
    "\n",
    "def get_top_percent_words(doc, percent):\n",
    "    # Returns a list of \"top-n\" most frequent words in a list\n",
    "    top_n = int(percent * len(set(doc)))\n",
    "    counter = Counter(doc).most_common(top_n)\n",
    "    top_n_words = [x[0] for x in counter]\n",
    "    # print(top_n_words)\n",
    "    return top_n_words\n",
    "\n",
    "def clean_document(doc):\n",
    "    spell = SpellChecker()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Lemmatize words (needed for calculating frequencies correctly )\n",
    "    doc = [lemmatizer.lemmatize(x) for x in doc]\n",
    "\n",
    "    # Get the top 10% of all words. This may include \"misspelled\" words\n",
    "    top_n_words = get_top_percent_words(doc, 0.1)\n",
    "\n",
    "    # Get a list of misspelled words\n",
    "    misspelled = spell.unknown(doc)\n",
    "\n",
    "    # Accept the correctly spelled words and top_n words\n",
    "    clean_words = [x for x in doc if x not in misspelled or x in top_n_words]\n",
    "\n",
    "    # Try to split the misspelled words to generate good words (ex. \"lifeisstrange\" -> [\"life\", \"is\", \"strange\"])\n",
    "    words_to_split = [x for x in doc if x in misspelled and x not in top_n_words]\n",
    "    split_words = flatten_list([wordninja.split(x) for x in words_to_split])\n",
    "\n",
    "    # Some splits may be nonsensical, so reject them (\"llouis\" -> ['ll', 'ou', \"is\"])\n",
    "    clean_words.extend(spell.known(split_words))\n",
    "\n",
    "    return clean_words\n",
    "\n",
    "def get_log_likelihood(doc1, doc2):\n",
    "    doc1_counts = Counter(doc1)\n",
    "    doc1_freq = {\n",
    "        x: doc1_counts[x]/len(doc1)\n",
    "        for x in doc1_counts\n",
    "    }\n",
    "\n",
    "    doc2_counts = Counter(doc2)\n",
    "    doc2_freq = {\n",
    "        x: doc2_counts[x]/len(doc2)\n",
    "        for x in doc2_counts\n",
    "    }\n",
    "\n",
    "    doc_ratios = {\n",
    "        # 1 is added to prevent division by 0\n",
    "        x: math.log((doc1_freq[x] +1 )/(doc2_freq[x]+1))\n",
    "        for x in doc1_freq if x in doc2_freq\n",
    "    }\n",
    "\n",
    "    top_ratios = Counter(doc_ratios).most_common()\n",
    "    top_percent = int(0.1 * len(top_ratios))\n",
    "    return top_ratios[:top_percent]\n",
    "\n",
    "# Function to generate a document based on likelihood values for words\n",
    "def get_scaled_list(log_list):\n",
    "    counts = [int(x[1]*100000) for x in log_list]\n",
    "    words = [x[0] for x in log_list]\n",
    "    cloud = []\n",
    "    for i, word in enumerate(words):\n",
    "        cloud.extend([word]*counts[i])\n",
    "    # Shuffle to make it more \"real\"\n",
    "    random.shuffle(cloud)\n",
    "    return cloud"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_smart_clouds(df):\n",
    "\n",
    "    neg_doc = flatten_list(df[df['sentiment']=='negative']['words'])\n",
    "    neg_doc = [x for x in neg_doc if is_acceptable(x)]\n",
    "\n",
    "    pos_doc = flatten_list(df[df['sentiment']=='positive']['words'])\n",
    "    pos_doc = [x for x in pos_doc if is_acceptable(x)]\n",
    "\n",
    "    neu_doc = flatten_list(df[df['sentiment']=='neutral']['words'])\n",
    "    neu_doc = [x for x in neu_doc if is_acceptable(x)]\n",
    "\n",
    "    # Clean all the documents\n",
    "    neg_doc_clean = clean_document(neg_doc)\n",
    "    neu_doc_clean = clean_document(neu_doc)\n",
    "    pos_doc_clean = clean_document(pos_doc)\n",
    "\n",
    "    # Combine classes B and C to compare against A (ex. \"positive\" vs \"non-positive\")\n",
    "    top_neg_words = get_log_likelihood(neg_doc_clean, flatten_list([pos_doc_clean, neu_doc_clean]))\n",
    "    top_neu_words = get_log_likelihood(neu_doc_clean, flatten_list([pos_doc_clean, neg_doc_clean]))\n",
    "    top_pos_words = get_log_likelihood(pos_doc_clean, flatten_list([neu_doc_clean, neg_doc_clean]))\n",
    "\n",
    "    # Generate syntetic a corpus using our loglikelihood values\n",
    "    neg_doc_final = get_scaled_list(top_neg_words)\n",
    "    neu_doc_final = get_scaled_list(top_neu_words)\n",
    "    pos_doc_final = get_scaled_list(top_pos_words)\n",
    "\n",
    "    # Visualise our synthetic corpus\n",
    "    fig = generate_word_clouds(neg_doc_final, neu_doc_final, pos_doc_final)\n",
    "    return fig\n",
    "\n",
    "# Convert string to a list of words\n",
    "wordcloud_df = df\n",
    "wordcloud_df['words'] = wordcloud_df.text.apply(lambda x:re.findall(r'\\w+', x ))\n",
    "get_smart_clouds(wordcloud_df).savefig(\"sentiment_wordclouds.png\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wordcloud_df = pfizy_df\n",
    "wordcloud_df['words'] = wordcloud_df.text.apply(lambda x:re.findall(r'\\w+', x ))\n",
    "get_smart_clouds(wordcloud_df).savefig(\"pfizy_sentiment_wordclouds.png\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}